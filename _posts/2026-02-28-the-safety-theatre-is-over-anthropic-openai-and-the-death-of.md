Let me research the most significant AI/technology developments from the past week.This is a remarkable story. Let me now write the post.

---
layout: post
title: "The Safety Theatre Is Over: Anthropic, OpenAI, and the Death of Voluntary AI Restraint"
date: 2026-02-28
category: "ai-technology"
tags: [anthropic, openai, pentagon, ai-safety, regulation, military-ai]
excerpt: "Anthropic has dropped its flagship safety pledge, OpenAI has erased 'safely' from its mission, and the Pentagon is blacklisting companies that refuse to hand over their AI without conditions — the experiment in voluntary AI restraint is finished."
---

The last company that could plausibly claim to be the responsible actor in artificial intelligence abandoned that claim this week. Anthropic — founded explicitly as the safety-conscious alternative to OpenAI — scrapped the central pillar of its Responsible Scaling Policy: the promise to never train or deploy an AI system unless it could guarantee adequate safety measures in advance. The same week, the Pentagon designated Anthropic a national security supply chain risk — a tool historically reserved for foreign adversaries like Huawei — for refusing to allow its models to be used without restrictions for mass surveillance and autonomous weapons. Within hours, OpenAI swooped in and signed a deal with the Pentagon containing ostensibly similar safeguards. The message to every AI company is now unmistakable: fall in line, or be destroyed.

## What Anthropic Actually Did

Anthropic committed in 2023 to never train an AI system unless it could guarantee in advance that its safety measures were adequate. For years, its leaders touted that promise — the central pillar of their Responsible Scaling Policy — as evidence that they would withstand market incentives to rush development of potentially dangerous technology.

On Tuesday, Anthropic revealed a new version of its RSP that drops that core commitment: to stop training and refuse to deploy an AI system if it couldn't guarantee it had proper safety guardrails in place.

The replacement is softer in every way. Under the new policy, Anthropic pledges to publish "Frontier Safety Roadmaps" outlining planned safety milestones and regular "Risk Reports." The company says it will match or exceed competitors' safety efforts and delay development if it both leads the field and identifies significant catastrophic risk. What it will no longer do is promise to halt training until all mitigations are guaranteed in advance.

The framing is classic corporate restructuring of an ethical retreat as a strategic advance. An Anthropic spokesperson described the updated policy as "the strongest to date on the level of public accountability and transparency." But the actual mechanism of constraint — the hard stop, the thing that made the old policy meaningful — is gone. The company acknowledged the new framework is more flexible: "Rather than being hard commitments, these are public goals that we will openly grade our progress towards."

## The Three Justifications — and What They Conceal

Anthropic cited three reasons for the change. A "zone of ambiguity" muddling the public case for risk from capability thresholds, an increasingly anti-regulatory political climate, and requirements at higher RSP levels that are very hard to meet without industry-wide coordination.

The third reason is the one that matters — and the one that exposes the structural failure at the heart of voluntary AI safety. Anthropic's original theory was that by imposing strict standards on itself, it would trigger a "race to the top" where competitors adopted similar frameworks. Anthropic wrote that it had hoped its original safety principles "would encourage other AI companies to introduce similar policies." The company now suggests that hasn't played out.

This is an honest admission, and it deserves credit for honesty. But the conclusion Anthropic draws — that it should therefore weaken its own commitments — does not follow. As the Transparency Coalition put it bluntly: "That's the problem with company policies. They change. Sometimes overnight. Laws require all competitors to play by the same rules. They exist so that basic and appropriate product safety standards don't stand or fall on the capricious decision of a single CEO."

## The Pentagon Confrontation

The timing is, to put it gently, difficult to ignore. On the same day Anthropic published its weakened safety policy, Defense Secretary Pete Hegseth gave Anthropic CEO Dario Amodei an ultimatum to roll back the company's AI safeguards or risk losing a $200 million Pentagon contract. The Pentagon threatened to put Anthropic on what is effectively a government blacklist.

Anthropic insists the two events are unrelated. Perhaps. But the week's events escalated with remarkable speed. Anthropic — the only AI firm whose model was deployed on the Pentagon's classified networks — sought guardrails preventing its technology from being used for mass surveillance of Americans or military operations without human approval. But the Pentagon insisted any deal should allow use for "all lawful purposes."

When Anthropic refused to budge by the Friday deadline, Hegseth wrote: "Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic." The designation is typically imposed on foreign adversaries.

The rhetoric from the administration was punitive and personal. Hegseth called it a "cowardly act of corporate virtue-signaling that places Silicon Valley ideology above American lives." Defense officials and President Trump were irate at the idea that Anthropic could have any say over how the Pentagon uses technology. "WE will decide the fate of our Country — NOT some out-of-control, Radical Left AI company," Trump posted on Truth Social.

On Friday night, Anthropic announced it would sue the Pentagon. Anthropic called the designation "legally unsound" and warned it would set a "dangerous precedent for any American company that negotiates with the government."

## OpenAI Steps In — With the Same Red Lines

Here is where the story turns from dramatic to absurd. Sam Altman said late Friday night that OpenAI had reached an agreement with the Pentagon to use its AI models, after the Defense Department agreed to safety red lines similar to Anthropic's. Altman wrote: "Two of our most important safety principles are prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems. The DoW agrees with these principles, reflects them in law and policy, and we put them into our agreement."

Read that again. The Pentagon blacklisted Anthropic for insisting on restrictions against mass surveillance and autonomous weapons. Then it accepted those same restrictions from OpenAI. Hours after blacklisting Anthropic, the Pentagon accepted OpenAI's proposed safety framework — one that contained the same two red lines.

This strongly suggests the dispute was never really about the specific safety conditions. It was about who gets to set terms — and about punishing a company the administration views as ideologically hostile.

## The Broader Pattern: Safety as a Disposable Brand

This week's events must be understood alongside a parallel development. When OpenAI restructured into a for-profit company, it removed all safety language from its mission statement. Its IRS filing now reads simply: "OpenAI's mission is to ensure that artificial general intelligence benefits all of humanity" — removing the word "safely" that was found in every filing previously.

According to Platformer, OpenAI has also disbanded its "mission alignment" team. OpenAI did not just change PR copy. It changed the mission statement recorded in IRS filings, at the same time it dissolved multiple safety-labelled teams, saw senior safety researchers depart, and accelerated its financing and corporate structure toward scale.

And earlier this month, Mrinank Sharma, a senior safety researcher, said he was leaving Anthropic. "I continuously find myself reckoning with our situation," he wrote. "The world is in peril."

The pattern across the industry is now undeniable. OpenAI dropped "safely" from its mission. Anthropic dropped its hard safety commitments from its RSP. Within months of Anthropic's original policy, both OpenAI and Google DeepMind had adopted broadly similar frameworks. A rollback by the policy's originator may reshape what "responsible scaling" means across the industry.

Meanwhile, since Trump's second term began, federal policy has emphasised "innovation-first" approaches while states continue to pass enforceable AI rules that take effect in 2026. But Trump signed an executive order that essentially declared war on state-level AI laws, directing the Attorney General to establish an AI Litigation Task Force to challenge state AI laws deemed inconsistent with federal policy.

In other words: the companies are shedding their voluntary commitments. The federal government is actively attacking the state-level regulations that might have replaced them. And nobody is filling the vacuum.

## What This Actually Means

The experiment in voluntary AI safety restraint has ended. Not with a gradual erosion but with a week in which the industry's self-declared safety leader scrapped its defining commitment, while the US government demonstrated it will use the full coercive apparatus of the state — supply chain blacklisting, the Defense Production Act, presidential directives — against any company that tries to impose its own ethical limits on how its technology is deployed.

The implications are structural. The Pentagon's move is a signal to every AI company looking to sell services to the government: make sure you do not attempt to put any sort of restrictions on AI's uses. Eight of the ten largest US companies currently use Claude, including defense contractors, cloud providers, banks, and consulting firms. Every general counsel at every Fortune 500 firm with Pentagon exposure now faces the same question: is using Claude worth the legal uncertainty?

There is a genuine question buried in this mess about who ought to set limits on military AI — private companies or democratic governments. That question deserves serious engagement. But what happened this week was not a principled assertion of democratic authority over AI governance. It was an administration using economic coercion to punish a company for holding positions that the same administration then accepted from a more compliant competitor.

The honest framing is this: safety in AI development is now entirely contingent on market incentives and political alignment. There are no binding constraints on what frontier AI companies build or how they deploy it. The companies that once claimed to provide those constraints have abandoned them. The government that might have imposed them through regulation is instead dismantling the regulatory infrastructure that exists, while simultaneously demanding unrestricted access to the technology. The reader should understand that when any AI company now invokes "safety" or "responsibility," what they mean is: we will do what is commercially and politically convenient, and we will describe that as safety.
---