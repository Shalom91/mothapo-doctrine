---
layout: post
title: "The Safety Exits: How AI's Last Guardrails Fell in a Single Week"
date: 2026-02-28
category: "ai-technology"
tags: [anthropic, openai, ai-safety, pentagon, regulation]
excerpt: "In the same month, OpenAI deleted 'safely' from its mission and Anthropic dropped its pledge to halt dangerous training — the two companies that were supposed to be the adults in the room have abandoned the room entirely."
---

The story that matters this week is not a product launch. It is the coordinated collapse of the last voluntary safety commitments in frontier AI development, happening under simultaneous pressure from capital markets and the United States government.

In February 2026, OpenAI removed "safely" from its mission statement. The same month, Anthropic dropped its pledge to pause training if safety couldn't keep up. Anthropic was founded because OpenAI wasn't safe enough. It took five years for both companies to arrive at the same destination: safety as a negotiable feature, not a constraint.

## What OpenAI Did

The change was not announced at a keynote. OpenAI used to say it aimed to build artificial intelligence that "safely benefits humanity, unconstrained by a need to generate financial return." While reviewing its latest IRS disclosure form, released in November 2025 covering 2024, a scholar noticed OpenAI had removed "safely" from its mission statement. That change in wording coincided with its transformation from a nonprofit into a business increasingly focused on profits.

The financial context makes the motive plain. As of early February 2026, the company was in talks with SoftBank for an additional $30 billion and stands to get up to a total of $60 billion from Amazon, Nvidia, and Microsoft combined. OpenAI is now valued at over $500 billion, up from $300 billion in March 2025. The new structure also paves the way for an eventual initial public offering, which would increase the pressure to make money for its shareholders.

OpenAI framed this as a routine update. The company said it "rephrased our mission" and that "the words and approach changed to serve the same goal — benefiting humanity." But the people closest to the work have been saying something different for a long time. They raised tens of billions. Disbanded multiple safety teams. Lost senior safety researchers. Then removed the most important word from the one document they filed with the U.S. government.

A Tufts University professor who studies nonprofit governance put it directly: given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it. Furthermore, since all but one board member currently serves on both boards, it is hard to see how they might oversee themselves.

## What Anthropic Did

Anthropic's move was louder but no less significant. In 2023, Anthropic committed to never train an AI system unless it could guarantee in advance that the company's safety measures were adequate. For years, its leaders touted that promise as evidence that they are a responsible company that would withstand market incentives to rush to develop a potentially dangerous technology. But in recent months the company decided to radically overhaul the RSP. That decision included scrapping the promise not to release AI models if Anthropic can't guarantee proper risk mitigations in advance.

The justification offered by Anthropic's chief science officer, Jared Kaplan, has become the defining quote of the week: "We felt that it wouldn't actually help anyone for us to stop training AI models. We didn't really feel, with the rapid advance of AI, that it made sense for us to make unilateral commitments … if competitors are blazing ahead."

The new policy replaces hard tripwires with something far softer. The new version replaces that hard limit with a two-part condition: Anthropic promises to delay development only if leaders both consider the company to be the leader of the AI race and judge the risks of catastrophe to be material. Critics note that both conditions must be true simultaneously — a threshold considerably harder to trigger than the categorical rule it replaces.

A commitment to publish is structurally different from a commitment to stop. That single sentence captures the entire shift. What Anthropic now offers is transparency without constraint — regular reports about risk, public scorecards of its own progress, all of which it grades itself. What it no longer offers is any binding mechanism to actually slow down.

The METR policy director who reviewed a draft of the new policy warned of a "frog-boiling" effect, where danger slowly ramps up without a single moment that sets off alarms.

## The Pentagon Dimension

The timing is not coincidental. Anthropic enjoys a $200 million contract with the Pentagon it signed last summer to deploy Claude across the military. But that critical money faucet is now in jeopardy, as Trump officials reportedly threatened to cut off Anthropic over the company's insistence that its tech shouldn't be used for mass surveillance and autonomous weaponry.

This week, the confrontation reached its conclusion. President Trump said Friday the U.S. government would blacklist Anthropic, and the Pentagon declared the company a "supply chain risk," in the most consequential and controversial policy decision to date at the intersection of artificial intelligence and national security. Anthropic rebuffed the Pentagon's demand to lift all safeguards on the military's use of its model, Claude, due to its concerns about the use of AI for mass domestic surveillance and the development of weapons that fire without human involvement. For that, the government will now impose a penalty usually reserved for companies from adversarial countries, such as Chinese tech giant Huawei.

Trump wrote on Truth Social: "I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic's technology." This is particularly extraordinary because Claude is the only AI model currently used in the military's classified systems.

Hours later, OpenAI stepped into the gap. OpenAI CEO Sam Altman said late Friday that his company has agreed to terms with the Department of Defense on use of its AI models. "Tonight, we reached an agreement with the Department of War to deploy our models in their classified network." Altman claimed the deal preserved the same red lines Anthropic held — but Altman's deal with the Pentagon differs from Anthropic's, where there is concern existing law doesn't contemplate AI. OpenAI's position is that existing legal protections are sufficient. The restrictions in the agreement reflect existing U.S. law and the Pentagon's policies. Anthropic contends the law has not caught up with AI and worries AI can supercharge the legal collection of publicly available data, from social media posts to geolocation.

This is a genuine and important disagreement. Anthropic believes the current legal framework is inadequate for the AI era — that what is technically "lawful" in terms of data aggregation becomes something qualitatively different when processed at AI scale. OpenAI accepts the existing legal framework as sufficient. The Pentagon's position is simpler still: once it buys a tool, it has its own standards and procedures to determine whether and how to use it.

## The Structural Picture

The Transparency Coalition, an AI advocacy group, captured the underlying logic clearly: Back in 2023 and 2024, the major AI companies made widely publicized commitments to develop AI with a focus on "safety, security, and trust." Those commitments have now been largely abandoned and forgotten. That's the problem with company policies. They change. Sometimes overnight. Laws and legal standards may also change, but that change almost always requires public discussion, deliberation, study, and negotiation. It's slow. It's designed to be slow, in order to get it right. Laws require all competitors to play by the same rules. They exist so that basic and appropriate product safety standards don't stand or fall on the capricious decision of a single CEO.

Meanwhile, the regulatory landscape that might backstop voluntary commitments is itself under siege. On December 11, 2025, President Trump signed an executive order that essentially declared war on state-level AI laws. The order directs the Attorney General to establish an AI Litigation Task Force whose sole responsibility is to challenge state AI laws inconsistent with federal policy, and orders the Commerce Department to identify "burdensome state AI laws" within ninety days. There is still no overarching "AI Act" at the federal level.

The result: Anthropic itself noted that "despite rapid advances in AI capabilities over the past three years, government action on AI safety has moved slowly. The policy environment has shifted toward prioritizing AI competitiveness and economic growth, while safety-oriented discussions have yet to gain meaningful traction at the federal level."

## What This Means

What happened this week is not a technical development. It is a governance event. The two AI companies that positioned themselves as the responsible actors in the industry — one founded on safety, the other originally structured as a nonprofit to prevent exactly the dynamics now unfolding — have both abandoned their core safety constraints in the same month. One did it for capital markets. The other did it under competitive pressure and, almost certainly, with an awareness that the U.S. government was simultaneously threatening to blacklist it for maintaining ethical red lines on military use.

The honest framing is this: there are now no binding safety constraints on frontier AI development anywhere in the world. The EU AI Act's high-risk system rules do not apply until August 2026. U.S. state laws are being actively contested by the federal government. And the voluntary commitments that were supposed to bridge the gap — the ones that companies pointed to when asked why regulation wasn't urgent — are gone.

METR's Chris Painter said the new RSP shows that Anthropic "believes it needs to shift into triage mode with its safety plans, because methods to assess and mitigate risk are not keeping up with the pace of capabilities. This is more evidence that society is not prepared for the potential catastrophic risks posed by AI."

The industry's own safety organisations are now openly saying the gap between capability and control is widening. The companies building the systems are now openly saying they cannot afford to wait for safety to catch up. And the government that could mandate safety is instead threatening to destroy companies that try to maintain it voluntarily.

Read that paragraph again. That is where we are.